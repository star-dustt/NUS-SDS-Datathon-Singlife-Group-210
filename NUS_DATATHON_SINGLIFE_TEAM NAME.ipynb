{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The cell below is for you to keep track of the libraries used and install those libraries quickly\n",
    "##### Ensure that the proper library names are used and the syntax of `%pip install PACKAGE_NAME` is followed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas \n",
    "#%pip install matplotlib\n",
    "# add commented pip installation lines for packages used as shown above for ease of testing\n",
    "# the line should be of the format %pip install PACKAGE_NAME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DO NOT CHANGE** the filepath variable\n",
    "##### Instead, create a folder named 'data' in your current working directory and \n",
    "##### have the .parquet file inside that. A relative path *must* be used when loading data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can have as many cells as you want for code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "filepath = \"./data/catB_train.parquet\" \n",
    "# the initialised filepath MUST be a relative path to a folder named data that contains the parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ALL** Code for machine learning and dataset analysis should be entered below. \n",
    "##### Ensure that your code is clear and readable.\n",
    "##### Comments and Markdown notes are advised to direct attention to pieces of code you deem useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###...code...###\n",
    "df = pd.read_parquet(filepath)\n",
    "#print(df.head())\n",
    "\n",
    "\n",
    "#print(len(df['clntnum'].unique().tolist()))\n",
    "# Number of unique client numbers is the same as number of rows in df, ie every row corresponds to a unique person's data, so no grouping required\n",
    "\n",
    "df.isna().sum()\n",
    "\n",
    "for c in df.columns:\n",
    "    if len(df[c].unique()) == 1:\n",
    "        #print(c, df[c].unique())\n",
    "        df.drop(c, axis = 1, inplace = True)\n",
    "\n",
    "df.drop('clntnum', axis = 1, inplace = True)\n",
    "\n",
    "# As shown, race_desc, ctrycode_desc have some NA values. Since they are relatively small, we will try and impute those for race_desc, as we\n",
    "# make the assumption that one's race, and thus culture, background, wil impact the way they view their needs and thus whether they need\n",
    "# insurance.\n",
    "\n",
    "# Since non-empty entries for ctrycode_desc take only one value, ie Singapore, we make the simplifying assumption that all persons in this\n",
    "# database are in Singapore, which is not unrealistic since there is no reason to purchase insurance in Singapore if one is not in Singapore.\n",
    "\n",
    "# Multiple flags for certain GI claims have no entries. As such, these columns will be dropped due to having no or negative impact \n",
    "# on further analysis. However, due to the intepretation of these columns, it is perhaps recommended to review the data collection process \n",
    "# for these claims, or the specifics of the lack of these claims by relevant experts, etc. to determine if there may be some other conclusion\n",
    "# to be drawn.\n",
    "\n",
    "# Similarly, many columns corresponding to specific insurance products, notably if they have ever been bought and the GI claim success rate, are\n",
    "# either empty or 0. Intepretation is limited due to minimal knowledge of company processes, but the data appears to suggest some policies are\n",
    "# redundant to the people or something else that may be of interest. These columns will be removed nonetheless.\n",
    "        \n",
    "# Many columns corresponding to APE, SUMIN, PREMPAID are also all 0. These correspond to monetary transcations between the company and clients.\n",
    "# As the goal here is to predict customer satisfaction and changeover rates, the main value provided by these columns are potential correlation\n",
    "# between prices and customer willingness to pay said price which relates to satisfaction and changeover. However, these are already captured \n",
    "# as part of the various flg_ ... _claim columns as we assume clients always consider these when deciding to buy the insurance. Thus they are\n",
    "# redundant and all columns of form ape_, sumin_ and prempaid_ will be removed.\n",
    "\n",
    "# We also remove clntnum since it is effectively an index and has no real impact.\n",
    "\n",
    "\n",
    "df['f_purchase_lh'].fillna(value = 0, inplace = True)\n",
    "#print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "for c in df.isna():\n",
    "    if df[c].isna().sum()/df.shape[0] > 0.4:\n",
    "        df.drop(c, axis = 1, inplace = True)\n",
    "\n",
    "y = df['f_purchase_lh']\n",
    "\n",
    "x_numeric = df.select_dtypes(include = [\"int64\", \"float64\"]).columns\n",
    "df[x_numeric] = df[x_numeric].apply(lambda x: x.fillna(x.median()))\n",
    "df.drop('f_purchase_lh', axis = 1, inplace = True)\n",
    "\n",
    "x_non_numeric = df.select_dtypes(exclude = [\"int64\", \"float64\"]).columns\n",
    "#onc = OrdinalEncoder(handle_unknown = \"use_encoded_value\", unknown_value = np.nan)\n",
    "#onc.fit(df[x_non_numeric])\n",
    "#trans = pd.DataFrame(pd.DataFrame(onc.transform(df[x_non_numeric])), copy)\n",
    "#imputer = KNNImputer(n_neighbors = 3)\n",
    "#trans = pd.DataFrame(imputer.fit_transform(pd.DataFrame(trans)))\n",
    "#df2 = pd.concat([df, trans], axis = 1)\n",
    "\n",
    "#df['f_purchase_lh'].fillna(value = 0, inplace = True)\n",
    "df.drop(columns = x_non_numeric, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Counter({0.0: 13809, 1.0: 584})\n",
      "After: Counter({1.0: 13809, 0.0: 13809})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print('Before:', Counter(y_train))\n",
    "X_train, y_train = SMOTE().fit_resample(X_train, y_train)\n",
    "print('After:', Counter(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 17992 entries, 19550 to 15795\n",
      "Data columns (total 60 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   flg_substandard                    17992 non-null  float64\n",
      " 1   flg_is_borderline_standard         17992 non-null  float64\n",
      " 2   flg_is_revised_term                17992 non-null  float64\n",
      " 3   flg_is_rental_flat                 17992 non-null  float64\n",
      " 4   flg_has_health_claim               17992 non-null  float64\n",
      " 5   flg_has_life_claim                 17992 non-null  float64\n",
      " 6   flg_gi_claim                       17992 non-null  float64\n",
      " 7   flg_is_proposal                    17992 non-null  float64\n",
      " 8   flg_with_preauthorisation          17992 non-null  float64\n",
      " 9   flg_is_returned_mail               17992 non-null  float64\n",
      " 10  is_consent_to_mail                 17992 non-null  float64\n",
      " 11  is_consent_to_email                17992 non-null  float64\n",
      " 12  is_consent_to_call                 17992 non-null  float64\n",
      " 13  is_consent_to_sms                  17992 non-null  float64\n",
      " 14  is_valid_dm                        17992 non-null  float64\n",
      " 15  is_valid_email                     17992 non-null  float64\n",
      " 16  is_housewife_retiree               17992 non-null  float64\n",
      " 17  is_sg_pr                           17992 non-null  float64\n",
      " 18  is_class_1_2                       17992 non-null  float64\n",
      " 19  is_dependent_in_at_least_1_policy  17992 non-null  float64\n",
      " 20  hh_size                            17992 non-null  float64\n",
      " 21  n_months_last_bought_products      17992 non-null  int64  \n",
      " 22  flg_latest_being_lapse             17992 non-null  int64  \n",
      " 23  flg_latest_being_cancel            17992 non-null  int64  \n",
      " 24  tot_inforce_pols                   17992 non-null  int64  \n",
      " 25  f_hold_839f8a                      17992 non-null  int64  \n",
      " 26  f_hold_e22a6a                      17992 non-null  int64  \n",
      " 27  f_hold_c4bda5                      17992 non-null  int64  \n",
      " 28  f_hold_ltc                         17992 non-null  int64  \n",
      " 29  f_hold_507c37                      17992 non-null  int64  \n",
      " 30  f_ever_bought_839f8a               17992 non-null  int64  \n",
      " 31  f_ever_bought_e22a6a               17992 non-null  int64  \n",
      " 32  f_ever_bought_c4bda5               17992 non-null  int64  \n",
      " 33  f_ever_bought_ltc                  17992 non-null  int64  \n",
      " 34  f_ever_bought_507c37               17992 non-null  int64  \n",
      " 35  f_ever_bought_gi                   17992 non-null  int64  \n",
      " 36  f_ever_bought_grp_6fc3e6           17992 non-null  int64  \n",
      " 37  f_ever_bought_grp_de05ae           17992 non-null  int64  \n",
      " 38  f_ever_bought_grp_945b5a           17992 non-null  int64  \n",
      " 39  f_ever_bought_grp_6a5788           17992 non-null  int64  \n",
      " 40  f_ever_bought_ltc_43b9d5           17992 non-null  int64  \n",
      " 41  f_ever_bought_grp_9cdedf           17992 non-null  int64  \n",
      " 42  f_ever_bought_grp_1581d7           17992 non-null  int64  \n",
      " 43  f_ever_bought_grp_22decf           17992 non-null  int64  \n",
      " 44  f_ever_bought_lh_507c37            17992 non-null  int64  \n",
      " 45  f_ever_bought_lh_839f8a            17992 non-null  int64  \n",
      " 46  f_ever_bought_inv_e9f316           17992 non-null  int64  \n",
      " 47  f_ever_bought_grp_caa6ff           17992 non-null  int64  \n",
      " 48  f_ever_bought_grp_fd3bfb           17992 non-null  int64  \n",
      " 49  f_ever_bought_lh_e22a6a            17992 non-null  int64  \n",
      " 50  f_ever_bought_grp_70e1dd           17992 non-null  int64  \n",
      " 51  f_ever_bought_grp_e04c3a           17992 non-null  int64  \n",
      " 52  f_ever_bought_grp_fe5fb8           17992 non-null  int64  \n",
      " 53  f_ever_bought_grp_94baec           17992 non-null  int64  \n",
      " 54  f_ever_bought_grp_e91421           17992 non-null  int64  \n",
      " 55  f_ever_bought_lh_f852af            17992 non-null  int64  \n",
      " 56  f_ever_bought_lh_947b15            17992 non-null  int64  \n",
      " 57  f_elx                              17992 non-null  int64  \n",
      " 58  f_mindef_mha                       17992 non-null  int64  \n",
      " 59  f_retail                           17992 non-null  int64  \n",
      "dtypes: float64(21), int64(39)\n",
      "memory usage: 8.4 MB\n",
      "(17992, 60)\n",
      "Epoch 1/30\n",
      "450/450 [==============================] - 1s 1ms/step - loss: 0.2292 - accuracy: 0.9361\n",
      "Epoch 2/30\n",
      "450/450 [==============================] - 0s 1ms/step - loss: 0.1531 - accuracy: 0.9593\n",
      "Epoch 3/30\n",
      "450/450 [==============================] - 0s 1ms/step - loss: 0.1479 - accuracy: 0.9591\n",
      "Epoch 4/30\n",
      "450/450 [==============================] - 0s 1ms/step - loss: 0.1460 - accuracy: 0.9596\n",
      "Epoch 5/30\n",
      "450/450 [==============================] - 0s 1ms/step - loss: 0.1436 - accuracy: 0.9596\n",
      "Epoch 6/30\n",
      "450/450 [==============================] - 0s 998us/step - loss: 0.1423 - accuracy: 0.9595\n",
      "Epoch 7/30\n",
      "450/450 [==============================] - 0s 1ms/step - loss: 0.1394 - accuracy: 0.9600\n",
      "Epoch 8/30\n",
      "450/450 [==============================] - 0s 1ms/step - loss: 0.1389 - accuracy: 0.9603\n",
      "Epoch 9/30\n",
      "450/450 [==============================] - 0s 1ms/step - loss: 0.1376 - accuracy: 0.9603\n",
      "Epoch 10/30\n",
      "450/450 [==============================] - 1s 1ms/step - loss: 0.1363 - accuracy: 0.9610\n",
      "Epoch 11/30\n",
      "450/450 [==============================] - 0s 1ms/step - loss: 0.1355 - accuracy: 0.9612\n",
      "Epoch 12/30\n",
      "450/450 [==============================] - 0s 1ms/step - loss: 0.1339 - accuracy: 0.9622\n",
      "Epoch 13/30\n",
      "450/450 [==============================] - 0s 1ms/step - loss: 0.1314 - accuracy: 0.9628\n",
      "Epoch 14/30\n",
      "450/450 [==============================] - 0s 1ms/step - loss: 0.1303 - accuracy: 0.9628\n",
      "Epoch 15/30\n",
      "450/450 [==============================] - 0s 1ms/step - loss: 0.1285 - accuracy: 0.9632\n",
      "Epoch 16/30\n",
      "450/450 [==============================] - 0s 1ms/step - loss: 0.1268 - accuracy: 0.9637\n",
      "Epoch 17/30\n",
      "450/450 [==============================] - 1s 1ms/step - loss: 0.1243 - accuracy: 0.9640\n",
      "Epoch 18/30\n",
      "450/450 [==============================] - 0s 1ms/step - loss: 0.1226 - accuracy: 0.9646\n",
      "Epoch 19/30\n",
      "450/450 [==============================] - 0s 1ms/step - loss: 0.1219 - accuracy: 0.9654\n",
      "Epoch 20/30\n",
      "450/450 [==============================] - 0s 1ms/step - loss: 0.1210 - accuracy: 0.9657\n",
      "Epoch 21/30\n",
      "450/450 [==============================] - 0s 1ms/step - loss: 0.1190 - accuracy: 0.9661\n",
      "Epoch 22/30\n",
      "450/450 [==============================] - 0s 1ms/step - loss: 0.1178 - accuracy: 0.9656\n",
      "Epoch 23/30\n",
      "450/450 [==============================] - 0s 1ms/step - loss: 0.1177 - accuracy: 0.9666\n",
      "Epoch 24/30\n",
      "450/450 [==============================] - 0s 1ms/step - loss: 0.1143 - accuracy: 0.9683\n",
      "Epoch 25/30\n",
      "450/450 [==============================] - 0s 1ms/step - loss: 0.1136 - accuracy: 0.9670\n",
      "Epoch 26/30\n",
      "450/450 [==============================] - 1s 1ms/step - loss: 0.1133 - accuracy: 0.9674\n",
      "Epoch 27/30\n",
      "450/450 [==============================] - 0s 1ms/step - loss: 0.1117 - accuracy: 0.9688\n",
      "Epoch 28/30\n",
      "450/450 [==============================] - 0s 1ms/step - loss: 0.1100 - accuracy: 0.9694\n",
      "Epoch 29/30\n",
      "450/450 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.9692\n",
      "Epoch 30/30\n",
      "450/450 [==============================] - 0s 1ms/step - loss: 0.1073 - accuracy: 0.9694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1305e1da310>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(df, y, test_size=0.2, random_state=0)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(units = 50, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(units = 40, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(units = 30, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(units = 20, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs = 30, batch_size = 32, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 0s 804us/step - loss: 0.1622 - accuracy: 0.9603\n",
      "0.9602667689323425\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_82 (Dense)            (None, 50)                3050      \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 40)                2040      \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 30)                1230      \n",
      "                                                                 \n",
      " dense_85 (Dense)            (None, 20)                620       \n",
      "                                                                 \n",
      " dense_86 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6961 (27.19 KB)\n",
      "Trainable params: 6961 (27.19 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "_, accuracy = model.evaluate(X_val, y_val)\n",
    "print(accuracy)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model():\n",
    "    new_model = tf.keras.models.load_model('my_model.keras')\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cell below is **NOT** to be removed\n",
    "##### The function is to be amended so that it accepts the given input (dataframe) and returns the required output (list). \n",
    "##### It is recommended to test the function out prior to submission\n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "##### The hidden_data parsed into the function below will have the same layout columns wise as the dataset *SENT* to you\n",
    "##### Thus, ensure that steps taken to modify the initial dataset to fit into the model are also carried out in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_hidden_data(hidden_data: pd.DataFrame) -> list:\n",
    "    '''DO NOT REMOVE THIS FUNCTION.\n",
    "\n",
    "The function accepts a dataframe as input and return an iterable (list)\n",
    "of binary classes as output.\n",
    "\n",
    "The function should be coded to test on hidden data\n",
    "and should include any preprocessing functions needed for your model to perform. \n",
    "    \n",
    "All relevant code MUST be included in this function.'''\n",
    "    list = ['flg_substandard', 'flg_is_borderline_standard', 'flg_is_revised_term',\n",
    "       'flg_is_rental_flat', 'flg_has_health_claim', 'flg_has_life_claim',\n",
    "       'flg_gi_claim', 'flg_is_proposal', 'flg_with_preauthorisation',\n",
    "       'flg_is_returned_mail', 'is_consent_to_mail', 'is_consent_to_email',\n",
    "       'is_consent_to_call', 'is_consent_to_sms', 'is_valid_dm',\n",
    "       'is_valid_email', 'is_housewife_retiree', 'is_sg_pr', 'is_class_1_2',\n",
    "       'is_dependent_in_at_least_1_policy', 'hh_size',\n",
    "       'n_months_last_bought_products', 'flg_latest_being_lapse',\n",
    "       'flg_latest_being_cancel', 'tot_inforce_pols', 'f_hold_839f8a',\n",
    "       'f_hold_e22a6a', 'f_hold_c4bda5', 'f_hold_ltc', 'f_hold_507c37',\n",
    "       'f_ever_bought_839f8a', 'f_ever_bought_e22a6a', 'f_ever_bought_c4bda5',\n",
    "       'f_ever_bought_ltc', 'f_ever_bought_507c37', 'f_ever_bought_gi',\n",
    "       'f_ever_bought_grp_6fc3e6', 'f_ever_bought_grp_de05ae',\n",
    "       'f_ever_bought_grp_945b5a', 'f_ever_bought_grp_6a5788',\n",
    "       'f_ever_bought_ltc_43b9d5', 'f_ever_bought_grp_9cdedf',\n",
    "       'f_ever_bought_grp_1581d7', 'f_ever_bought_grp_22decf',\n",
    "       'f_ever_bought_lh_507c37', 'f_ever_bought_lh_839f8a',\n",
    "       'f_ever_bought_inv_e9f316', 'f_ever_bought_grp_caa6ff',\n",
    "       'f_ever_bought_grp_fd3bfb', 'f_ever_bought_lh_e22a6a',\n",
    "       'f_ever_bought_grp_70e1dd', 'f_ever_bought_grp_e04c3a',\n",
    "       'f_ever_bought_grp_fe5fb8', 'f_ever_bought_grp_94baec',\n",
    "       'f_ever_bought_grp_e91421', 'f_ever_bought_lh_f852af',\n",
    "       'f_ever_bought_lh_947b15', 'f_elx', 'f_mindef_mha', 'f_retail']\n",
    "    x = hidden_data[list]\n",
    "    result = model.predict(x)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cell to check testing_hidden_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563/563 [==============================] - 1s 793us/step\n",
      "[[0.08646835]\n",
      " [0.04748528]\n",
      " [0.04005768]\n",
      " ...\n",
      " [0.03947005]\n",
      " [0.02105278]\n",
      " [0.05515887]]\n"
     ]
    }
   ],
   "source": [
    "# This cell should output a list of predictions.\n",
    "test_df = pd.read_parquet(filepath)\n",
    "test_df = test_df.drop(columns=[\"f_purchase_lh\"])\n",
    "print(testing_hidden_data(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please have the filename renamed and ensure that it can be run with the requirements above being met. All the best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
